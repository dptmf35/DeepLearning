{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 (NLP : Natural Language Processing) \n",
    ": 사람이 사용하는 말을 처리하는 작업\n",
    "- Text mining : 텍스트를 기반으로 자연어 처리를 하는 방법\n",
    "- 음성인식/처리 : 음성을 기반으로 자연어 처리를 하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 과정\n",
    "(1) 형태소 분석 : 형태소/단어 분리 작업 (Tokenization)\n",
    "   - 한글의 문제점 : 띄어쓰기가 필수가 아니라는 것, 의태어/의성어 표현으로 분리가 힘듦 - > 형태소 분리\n",
    "\n",
    "(2) 전처리 : Cleaning (오류 수정), 정규화 (유사 단어 통합), 어간/표제어 추출(대표단어, 어미 추출, 의미 추출, 내부 단어 분리 ..), \n",
    "    불용어 처리(필요없는 단어 제거) ..\n",
    "    \n",
    "(3) 텍스트를 숫자로 변경\n",
    "   - 빈도수 분석\n",
    "   - 정렬\n",
    "   - 인덱스 부여\n",
    "   - 워드 임베딩 (텍스트를 실수의 벡터로 변환)\n",
    "\n",
    "(4) 신경망 이용해 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화(Tokenization) : 문장 (corpus)로부터 단어를 분리하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nlkt\n",
    "import nltk \n",
    "# 자연어 처리 툴킷 (영어기반)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "\n",
    "# punkt : 토큰화 라이브러리 \n",
    "# wordnet : 표제어 추출 라이브러리 \n",
    "# stopwords : 불용어 처리 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글용 자연어 처리 라이브러리 설치\n",
    "#!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am a boy.', 'My name is Song.', 'My age is 20.']\n",
      "['I', 'am', 'a', 'boy', '.', 'My', 'name', 'is', 'Song', '.', 'My', 'age', 'is', '20', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 분리\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "corpus = 'I am a boy. My name is Song. My age is 20.'\n",
    "\n",
    "print(sent_tokenize(corpus))\n",
    "print(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['군침이 싹 도네.', '배고파 저녁은 뭐 먹지?']\n",
      "['군침이', '싹', '도네', '.', '배고파', '저녁은', '뭐', '먹지', '?']\n"
     ]
    }
   ],
   "source": [
    "corpus2 = '군침이 싹 도네. 배고파 저녁은 뭐 먹지?'\n",
    "\n",
    "print(sent_tokenize(corpus2))\n",
    "print(word_tokenize(corpus2)) # 한글은 형태소로 분리됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 분리 ( 명사, 형태소 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['군침이 싹 도네.', '배고파 저녁은 뭐 먹지?']\n",
      "['군침', '저녁', '뭐']\n",
      "['군침', '싹', '도', '저녁', '뭐']\n",
      "['군침', '이', '싹', '돌', '네', '.', '배고프', '아', '저녁', '은', '뭐', '먹', '지', '?']\n",
      "['군침', '이', '싹', '도', '네', '.', '배고파', '저녁', '은', '뭐', '먹지', '?']\n",
      "[('군침', 'NNG'), ('이', 'JKS'), ('싹', 'MAG'), ('돌', 'VV'), ('네', 'EFN'), ('.', 'SF'), ('배고프', 'VV'), ('아', 'ECS'), ('저녁', 'NNG'), ('은', 'JX'), ('뭐', 'NP'), ('먹', 'VV'), ('지', 'ECD'), ('?', 'SF')]\n",
      "[('군침', 'Noun'), ('이', 'Josa'), ('싹', 'Noun'), ('도', 'Noun'), ('네', 'Josa'), ('.', 'Punctuation'), ('배고파', 'Adjective'), ('저녁', 'Noun'), ('은', 'Josa'), ('뭐', 'Noun'), ('먹지', 'Verb'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Kkma, Hannanum, Komoran, Mecab, Okt, Twitter\n",
    "\n",
    "kkma = Kkma()\n",
    "okt = Okt()\n",
    "\n",
    "# 문장 분리\n",
    "print(kkma.sentences(corpus2))\n",
    "\n",
    "# 명사 분리\n",
    "print(kkma.nouns(corpus2))\n",
    "print(okt.nouns(corpus2))\n",
    "\n",
    "# 형태소로 분리\n",
    "print(kkma.morphs(corpus2))\n",
    "print(okt.morphs(corpus2))\n",
    "\n",
    "# 품사 태킹 (분리된 단어가 어떤 품사인지 표시)\n",
    "print(kkma.pos(corpus2))\n",
    "print(okt.pos(corpus2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정제(Cleaning)와 정규화(Normalization)\n",
    "- 정제 : 오타 수정\n",
    "- 정규화 : 정규식을 이용해서 단어를 전처리하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "김동원 부장이 말했습니다 내일 봅시다  어쩌라는 건가요 \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "corpus = '김동원 부장이 말했습니다. \"내일 봅시다\" ==> 어쩌라는 건가요 ?'\n",
    "\n",
    "# 한글(^ㄱ-ㅎㅏ-ㅣ가-힣) , 영문자(a-zA-Z), 빈 공백()을 제외(^)하고 \n",
    "word = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z ]')\n",
    "\n",
    "# corpus에서 해당 정규식에 맞는 부분을 ''로 변경\n",
    "print(word.sub('',corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어(stopword) 처리 : 의미없는 단어를 삭제하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨데', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.', '삼겹살', '만쉐']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus1 = '고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨데 삼겹살을 구울 때는 중요한 게 있지. 삼겹살 만쉐'\n",
    "\n",
    "# 단어를 분리\n",
    "tokens = word_tokenize(corpus1)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기를', '아무렇게나', '구우려고', '고기라고', '같은', '아니거든', '예컨데', '삼겹살을', '구울', '중요한', '삼겹살', '만쉐']\n"
     ]
    }
   ],
   "source": [
    "# 불용어\n",
    "stop_word = ['하면' , '안', '돼', '.','다','게','때는','있지']\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in tokens :\n",
    "    # 분리된 단어가 불용어에 없다면(불용어가 아니라면?)\n",
    "    if i not in stop_word :\n",
    "        result.append(i)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분리하고 불용어 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['를','고','하면','안','돼','.','라고','다','같은','게','예컨데','을','때','는','있지']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['고기', '아무렇게나', '구', '우려', '고기', '아니거든', '삼겹살', '구울', '중요한', '삼겹살', '만쉐']\n"
     ]
    }
   ],
   "source": [
    "morphs = okt.morphs(corpus1)\n",
    "\n",
    "new_result = []\n",
    "\n",
    "for i in morphs :\n",
    "    if i not in stop_words :\n",
    "        new_result.append(i)\n",
    "        \n",
    "print(new_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코딩 : 단어를 숫자로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'고기': 1, '삼겹살': 2, '아무렇게나': 3, '구': 4, '우려': 5, '아니거든': 6, '구울': 7, '중요한': 8, '만쉐': 9}\n",
      "OrderedDict([('고기', 2), ('아무렇게나', 1), ('구', 1), ('우려', 1), ('아니거든', 1), ('삼겹살', 2), ('구울', 1), ('중요한', 1), ('만쉐', 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "# 단어의 빈도수를 분석해서 정렬하고 인덱스를 부여\n",
    "t.fit_on_texts(new_result)\n",
    "\n",
    "# 부여된 인덱스 확인\n",
    "print(t.word_index)\n",
    "\n",
    "# 빈도수 확인\n",
    "print(t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [3], [4], [5], [1], [6], [2], [7], [8], [2], [9]]\n",
      "['고기', '아무렇게나', '구', '우려', '고기', '아니거든', '삼겹살', '구울', '중요한', '삼겹살', '만쉐']\n"
     ]
    }
   ],
   "source": [
    "# 단어를 부여된 인덱스로 치환\n",
    "print(t.texts_to_sequences(new_result))\n",
    "print(new_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다음 문장으로 인코딩 해보세요\n",
    "[소크라테스가 말했습니다. \"너 자신을 알라\" ==> 이건 무슨 의미일까요? 글쎄요 그냥 소크라테스의 생각이 아닐까요. 아무 의미가 없어요. ^^ 자신 자신]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규식으로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소크라테스가 말했습니다 너 자신을 알라  이건 무슨 의미일까요 글쎄요 그냥 소크라테스의 생각이 아닐까요 아무 의미가 없어요  자신 자신\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "corpus = '소크라테스가 말했습니다. \"너 자신을 알라\" ==> 이건 무슨 의미일까요? 글쎄요 그냥 소크라테스의 생각이 아닐까요. 아무 의미가 없어요. ^^ 자신 자신'\n",
    "\n",
    "# 한글(^ㄱ-ㅎㅏ-ㅣ가-힣) , 영문자(a-zA-Z), 빈 공백()을 제외(^)하고 \n",
    "word = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z ]')\n",
    "\n",
    "print(word.sub('',corpus))\n",
    "\n",
    "corpus = word.sub('',corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형태소 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['소크라테스',\n",
       " '가',\n",
       " '말',\n",
       " '했습니다',\n",
       " '너',\n",
       " '자신',\n",
       " '을',\n",
       " '알라',\n",
       " '이건',\n",
       " '무슨',\n",
       " '의미',\n",
       " '일까',\n",
       " '요',\n",
       " '글쎄요',\n",
       " '그냥',\n",
       " '소크라테스',\n",
       " '의',\n",
       " '생각',\n",
       " '이',\n",
       " '아닐까',\n",
       " '요',\n",
       " '아무',\n",
       " '의미',\n",
       " '가',\n",
       " '없어요',\n",
       " '자신',\n",
       " '자신']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp = okt.morphs(corpus)\n",
    "corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['소크라테스', '너', '자신', '알라', '무슨', '의미', '글쎄요', '그냥', '소크라테스', '생각', '아닐까', '아무', '의미', '없어요', '자신', '자신']\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = ['가','을', '말', '했습니다','이건','일까','요','의','이','가']\n",
    "results = []\n",
    "\n",
    "for i in corp :\n",
    "    if i not in stopwords_list :\n",
    "        results.append(i)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'자신': 1, '소크라테스': 2, '의미': 3, '너': 4, '알라': 5, '무슨': 6, '글쎄요': 7, '그냥': 8, '생각': 9, '아닐까': 10, '아무': 11, '없어요': 12}\n",
      "OrderedDict([('소크라테스', 2), ('너', 1), ('자신', 3), ('알라', 1), ('무슨', 1), ('의미', 2), ('글쎄요', 1), ('그냥', 1), ('생각', 1), ('아닐까', 1), ('아무', 1), ('없어요', 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "t.fit_on_texts(results)\n",
    "\n",
    "print(t.word_index)\n",
    "print(t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [4], [1], [5], [6], [3], [7], [8], [2], [9], [10], [11], [3], [12], [1], [1]]\n",
      "['소크라테스', '너', '자신', '알라', '무슨', '의미', '글쎄요', '그냥', '소크라테스', '생각', '아닐까', '아무', '의미', '없어요', '자신', '자신']\n"
     ]
    }
   ],
   "source": [
    "print(t.texts_to_sequences(results))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW(Bag of Word) : 단어사전\n",
    "- 단어의 순서는 고려하지 않고 빈도수만 고려한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['소크라테스',\n",
       " '너',\n",
       " '자신',\n",
       " '알라',\n",
       " '무슨',\n",
       " '의미',\n",
       " '글쎄요',\n",
       " '그냥',\n",
       " '소크라테스',\n",
       " '생각',\n",
       " '아닐까',\n",
       " '아무',\n",
       " '의미',\n",
       " '없어요',\n",
       " '자신',\n",
       " '자신']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 빈도수 분석 -> 정렬 -> 인덱스 부여 -> 정수로 변환 -> 원핫 인코딩\n",
    "cv = CountVectorizer()\n",
    "\n",
    "print(cv.fit_transform(results).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그냥', '글쎄요', '무슨', '생각', '소크라테스', '아닐까', '아무', '알라', '없어요', '의미', '자신']\n"
     ]
    }
   ],
   "source": [
    "# 분리된 단어 확인\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'소크라테스': 4, '자신': 10, '알라': 7, '무슨': 2, '의미': 9, '글쎄요': 1, '그냥': 0, '생각': 3, '아닐까': 5, '아무': 6, '없어요': 8}\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 확인\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1]]\n",
      "['그냥', '글쎄요', '무슨', '생각', '소크라테스', '아닐까', '알라', '없어요', '의미', '자신']\n",
      "{'소크라테스': 4, '자신': 9, '알라': 6, '무슨': 2, '의미': 8, '글쎄요': 1, '그냥': 0, '생각': 3, '아닐까': 5, '없어요': 7}\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리\n",
    "stop_word2 = ['아무']\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_word2)\n",
    "\n",
    "print(cv.fit_transform(results).toarray())\n",
    "print(cv.get_feature_names())\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 빈도수 분석\n",
    "- TF (Term Frequency) : 전체 문장에서 단어가 몇 번 나왔는지 계산\n",
    "- DF (Document Frequency) : 해당 문서나 문장에서 단어가 몇 번 나왔는지 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "['그냥', '글쎄요', '무슨', '생각', '소크라테스', '아닐까', '아무', '알라', '없어요', '의미', '자신']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(results)\n",
    "\n",
    "print(tfidf.transform(results).toarray())\n",
    "print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp2 = ['여러분 지금은 수업중입니다',\n",
    "        '마지막 시간인데 텍스트마이닝 너무 어렵답니다',\n",
    "        '텍스트마이닝 수업 끝나고 집에 가고 싶어요. 여러분',\n",
    "        '지금은 수업중']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.66767854 0.         0.         0.         0.52640543 0.52640543\n",
      "  0.         0.        ]\n",
      " [0.         0.         0.46516193 0.46516193 0.         0.\n",
      "  0.         0.46516193 0.         0.46516193 0.         0.\n",
      "  0.         0.36673901]\n",
      " [0.40021825 0.40021825 0.         0.         0.40021825 0.\n",
      "  0.         0.         0.40021825 0.         0.31553666 0.\n",
      "  0.40021825 0.31553666]\n",
      " [0.         0.         0.         0.         0.         0.78528828\n",
      "  0.         0.         0.         0.         0.         0.6191303\n",
      "  0.         0.        ]]\n",
      "['가고', '끝나고', '너무', '마지막', '수업', '수업중', '수업중입니다', '시간인데', '싶어요', '어렵답니다', '여러분', '지금은', '집에', '텍스트마이닝']\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer().fit(corp2)\n",
    "\n",
    "print(tfidf.transform(corp2).toarray())\n",
    "print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 유사도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df : 문장에서 등장하는 단어의 최소 빈도수\n",
    "# max_df : 문장에서 등장하는 단어의 최대 빈도수\n",
    "tfidf = TfidfVectorizer(min_df=1,max_df=4, decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 여러분 지금 은 수업 중 입니다', ' 마지막 시간 인데 텍스트 마 이닝 너무 어렵답니다', ' 텍스트 마 이닝 수업 끝나고 집 에 가고 싶어요 . 여러분', ' 지금 은 수업 중']\n"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 토큰화\n",
    "# 각 문장을 불러와서 문장 별로 형태소 분리를 해서 리스트에 저장\n",
    "content_tokens = [okt.morphs(row) for row in corp2]\n",
    "\n",
    "count_for_vectorize = []\n",
    "\n",
    "# 각 문장을 불러온다\n",
    "for content in content_tokens :\n",
    "    # 분리된 단어로 구성된 문장을 저장하기 위함\n",
    "    sentence = ''\n",
    "    \n",
    "    # 해당 문장에서 단어를 가져온다\n",
    "    for word in content :\n",
    "        # 해당 단어들로 구성된 문장을 생성\n",
    "        sentence = sentence + ' ' + word\n",
    "    \n",
    "    # 생성된 문장을 저장\n",
    "    count_for_vectorize.append(sentence)\n",
    "    \n",
    "print(count_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 14)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf.fit_transform(count_for_vectorize)\n",
    "\n",
    "X.shape # 3개의 문장, 14개의 특성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가고', '끝나고', '너무', '마지막', '수업', '시간', '싶어요', '어렵답니다', '여러분', '이닝', '인데', '입니다', '지금', '텍스트']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 여러분 지금 은 수']\n"
     ]
    }
   ],
   "source": [
    "# 유사도 분석용 데이터\n",
    "corp_test = ['여러분 지금은 수']\n",
    "\n",
    "# 토큰화\n",
    "# 각 문장을 불러와서 문장 별로 형태소 분리를 해서 리스트에 저장\n",
    "test_content_tokens = [okt.morphs(row) for row in corp_test]\n",
    "\n",
    "test_count_for_vectorize = []\n",
    "\n",
    "for content in test_content_tokens :\n",
    "    sentence = ''\n",
    "    \n",
    "    for word in content :\n",
    "        sentence = sentence + ' ' + word\n",
    "    \n",
    "    test_count_for_vectorize.append(sentence)\n",
    "    \n",
    "print(test_count_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 14)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = tfidf.transform(test_count_for_vectorize)\n",
    "\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 문장의 유사도를 비교하는 함수\n",
    "import scipy as sp\n",
    "\n",
    "def distance(v1, v2) :\n",
    "    dis = v1 - v2\n",
    "    \n",
    "    # 유클리디안 거리 구하는 함수\n",
    "    return sp.linalg.norm(dis.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['여러분 지금은 수']\n",
      "0.7939128907063565\n",
      "여러분 지금은 수업중입니다\n",
      "1.4142135623730951\n",
      "마지막 시간인데 텍스트마이닝 너무 어렵답니다\n",
      "1.2306124202155906\n",
      "텍스트마이닝 수업 끝나고 집에 가고 싶어요. 여러분\n",
      "0.9491276476981877\n",
      "지금은 수업중\n"
     ]
    }
   ],
   "source": [
    "print(corp_test)\n",
    "for i in range(X.shape[0]) : # 데이터 갯수만큼\n",
    "    # 한 줄씩 문장을 가져온다\n",
    "    vec = X.getrow(i)\n",
    "    \n",
    "    # 두 데이터간의 거리를 구한다\n",
    "    d = distance(vec, X_test)\n",
    "    \n",
    "    print(d)\n",
    "    print(corp2[i])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
